{
 "metadata": {
  "name": "",
  "signature": "sha256:bc7ce2f0e52842811af9dc00623992d5079e7a91c90141e2f7630a84147833de"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import popplerqt4"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "filepath = '/Users/chbrown/work/tmp/pdfconv/nakassis.pdf'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "filepath = '/Users/chbrown/github/acl/anthology/P/P13/P13-1006.pdf'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "doc = popplerqt4.Poppler.Document.load(filepath)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Document has %d pages' % doc.numPages()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Document has 11 pages\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "doc.pageMode()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "0"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print dir(doc)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['AcroForm', 'Antialiasing', 'ArthurBackend', 'FormType', 'FullScreen', 'NoForm', 'NoLayout', 'OneColumn', 'OverprintPreview', 'PageLayout', 'PageMode', 'RenderBackend', 'RenderHint', 'RenderHints', 'SinglePage', 'SplashBackend', 'TextAntialiasing', 'TextHinting', 'TextSlightHinting', 'ThinLineShape', 'ThinLineSolid', 'TwoColumnLeft', 'TwoColumnRight', 'TwoPageLeft', 'TwoPageRight', 'UseAttach', 'UseNone', 'UseOC', 'UseOutlines', 'UseThumbs', 'XfaForm', '__class__', '__delattr__', '__dict__', '__doc__', '__format__', '__getattribute__', '__hash__', '__init__', '__module__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'availableRenderBackends', 'colorDisplayProfile', 'colorRgbProfile', 'date', 'embeddedFiles', 'fontData', 'fonts', 'formType', 'getPdfId', 'getPdfVersion', 'hasEmbeddedFiles', 'hasOptionalContent', 'info', 'infoKeys', 'isEncrypted', 'isLinearized', 'isLocked', 'linkDestination', 'load', 'loadFromData', 'metadata', 'newFontIterator', 'numPages', 'okToAddNotes', 'okToAssemble', 'okToChange', 'okToCopy', 'okToCreateFormFields', 'okToExtractForAccessibility', 'okToFillForm', 'okToPrint', 'okToPrintHighRes', 'optionalContentModel', 'page', 'pageLayout', 'pageMode', 'paperColor', 'pdfConverter', 'psConverter', 'renderBackend', 'renderHints', 'scripts', 'setColorDisplayProfile', 'setColorDisplayProfileName', 'setPaperColor', 'setRenderBackend', 'setRenderHint', 'toc', 'unlock']\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "page = doc.page(4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "texts = "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for t in page.textList():\n",
      "    print t.text()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "two\n",
        "lattices.\n",
        "This\n",
        "jointly\n",
        "selects\n",
        "the\n",
        "optimal\n",
        "detec-\n",
        "tions\n",
        "to\n",
        "form\n",
        "the\n",
        "track,\n",
        "together\n",
        "with\n",
        "the\n",
        "optimal\n",
        "state\n",
        "sequence,\n",
        "and\n",
        "scores\n",
        "that\n",
        "combination.\n",
        "over,\n",
        "we\n",
        "wish\n",
        "the\n",
        "track\n",
        "to\n",
        "be\n",
        "temporally\n",
        "coherent;\n",
        "we\n",
        "want\n",
        "the\n",
        "objects\n",
        "in\n",
        "a\n",
        "track\n",
        "to\n",
        "move\n",
        "smoothly\n",
        "over\n",
        "time\n",
        "and\n",
        "not\n",
        "jump\n",
        "around\n",
        "the\n",
        "field\n",
        "of\n",
        "view.\n",
        "Let\n",
        "G(D\n"
       ]
      },
      {
       "ename": "UnicodeEncodeError",
       "evalue": "'ascii' codec can't encode character u'\\u2212' in position 1: ordinal not in range(128)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-20-02f30c156955>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mprint\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;31mUnicodeEncodeError\u001b[0m: 'ascii' codec can't encode character u'\\u2212' in position 1: ordinal not in range(128)"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# t1.text(), t1.boundingBox(), t1.charBoundingBox(1), t1.hasSpaceAfter(), t1.nextWord()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "(PyQt4.QtCore.QString(u'two'),\n",
        " PyQt4.QtCore.QRectF(307.27699999999993, 63.68599159999985, 16.25455900000003, 13.1454655),\n",
        " PyQt4.QtCore.QRectF(310.30972979999996, 63.68599159999985, 7.767279200000019, 13.1454655),\n",
        " True,\n",
        " <popplerqt4.TextBox at 0x10c272df8>)"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text1 = t1.text()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# http://www.cs.berkeley.edu/~dlwh/papers/spanparser.pdf\n",
      "spanparser_filepath = '/Users/chbrown/work/tmp/pdfconv/spanparser.pdf'\n",
      "spanparser_doc = popplerqt4.Poppler.Document.load(spanparser_filepath)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " # pages: 10\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "spanparser_page = spanparser_doc.page(0)\n",
      "spanparser_textlist = spanparser_page.textList()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def body(textboxes):\n",
      "    for textbox in textboxes:\n",
      "        yield unicode(textbox.text())\n",
      "        if textbox.hasSpaceAfter():\n",
      "            yield ' '\n",
      "\n",
      "print ''.join(body(spanparser_textlist))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Less Grammar, More FeaturesDavid HallGreg DurrettDan KleinComputer Science DivisionUniversity of California, Berkeley{dlwh,gdurrett,klein}@cs.berkeley.eduAbstractWe present a parser that relies primar-ily on extracting information directly fromsurface spans rather than on propagat-ing information through enriched gram-mar structure. For example, instead of cre-ating separate grammar symbols to markthe definiteness of an NP, our parser mightinstead capture the same information fromthe first word of the NP. Moving contextout of the grammar and onto surface fea-tures can greatly simplify the structuralcomponent of the parser: because so manydeep syntactic cues have surface reflexes,our system can still parse accurately withcontext-free backbones as minimal as X-bar grammars. Keeping the structuralbackbone simple and moving features tothe surface also allows easy adaptationto new languages and even to new tasks.On the SPMRL 2013 multilingual con-stituency parsing shared task (Seddah etal., 2013), our system outperforms the topsingle parser system of Bj\u00a8orkelund et al.(2013) on a range of languages. In addi-tion, despite being designed for syntacticanalysis, our system also achieves state-of-the-art numbers on the structural senti-ment task of Socher et al. (2013). Finally,we show that, in both syntactic parsing andsentiment analysis, many broad linguistictrends can be captured via surface features.1IntroductionNa\u00a8\u0131ve context-free grammars, such as those em-bodied by standard treebank annotations, do notparse well because their symbols have too littlecontext to constrain their syntactic behavior. Forexample, to PPs usually attach to verbs and ofPPs usually attach to nouns, but a context-free PPsymbol can equally well attach to either. Muchof the last few decades of parsing research hastherefore focused on propagating contextual in-formation from the leaves of the tree to inter-nal nodes. For example, head lexicalization (Eis-ner, 1996; Collins, 1997; Charniak, 1997), struc-tural annotation (Johnson, 1998; Klein and Man-ning, 2003), and state-splitting (Matsuzaki et al.,2005; Petrov et al., 2006) are all designed to takecoarse symbols like PP and decorate them withadditional context. The underlying reason thatsuch propagation is even needed is that PCFGparsers score trees based on local configurationsonly, and any information that is not threadedthrough the tree becomes inaccessible to the scor-ing function. There have been non-local ap-proaches as well, such as tree-substitution parsers(Bod, 1993; Sima\u2019an, 2000), neural net parsers(Henderson, 2003), and rerankers (Collins andKoo, 2005; Charniak and Johnson, 2005; Huang,2008). These non-local approaches can actuallygo even further in enriching the grammar\u2019s struc-tural complexity by coupling larger domains invarious ways, though their non-locality generallycomplicates inference.In this work, we instead try to minimize thestructural complexity of the grammar by movingas much context as possible onto local surface fea-tures. We examine the position that grammarsshould not propagate any information that is avail-able from surface strings, since a discriminativeparser can access that information directly. Wetherefore begin with a minimal grammar and it-eratively augment it with rich input features thatdo not enrich the context-free backbone. Previ-ous work has also used surface features in theirparsers, but the focus has been on machine learn-ing methods (Taskar et al., 2004), latent annota-tions (Petrov and Klein, 2008a; Petrov and Klein,2008b), or implementation (Finkel et al., 2008).By contrast, we investigate the extent to which\n"
       ]
      }
     ],
     "prompt_number": 19
    }
   ],
   "metadata": {}
  }
 ]
}